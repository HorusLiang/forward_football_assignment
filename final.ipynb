{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('PassEventsForwardFootball.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "# there will be a lot of columns that are not informative( just have one unique value), then check how many here\n",
    "def get_columns_with_one_unique_value(df):\n",
    "    col_counts = df.nunique()\n",
    "    cols_with_one_unique_value = col_counts[col_counts == 1]\n",
    "    return list(cols_with_one_unique_value.index)\n",
    "get_columns_with_one_unique_value(df)\n",
    "\n",
    "# *********************************************************\n",
    "\n",
    "# --------------------------------------------------------- Observe\n",
    "def print_unique_value(df):\n",
    "    for col in df:\n",
    "        print(\"column name:\",col)\n",
    "        print(df[col].unique())\n",
    "        print(\"---\")\n",
    "# print_unique_value(df)\n",
    "\n",
    "# from the result, we could see that columns\n",
    "# Type, x_pitchsize, y_pitchsize each have just one unique value->done\n",
    "# Club has just one unique vale ['Team Forward Football']-> done\n",
    "\n",
    "\n",
    "# isForward and isSucceeded are either True or False, then convert them to numerical value 0 and 1 in order to better process-> done\n",
    "# Team is either ['Team Forward Football_1' 'Team Forward Football_2'], then convert them to numerical value 0 and 1 -> done\n",
    "# Pass type ['Forward pass' 'Lateral pass' 'Backward pass']-> 0,1,2->done\n",
    "# Pressure level -> ['Full Pressure' 'No Pressure' 'Limited Pressure']->done\n",
    "# column name: Zone ['Attack' 'Defence' 'Mid field']->0,1,2\n",
    "# column name: Playing direction_first half ['left' 'right']->done\n",
    "# column name: Playing direction_second half ['left' 'right']->done\n",
    "\n",
    "# matchDuration is written in minutes and also just have two values-> don't need to convert\n",
    "\n",
    "# *********************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Process features\n",
    "\n",
    "# convert data type of caregorical columns to int for easier process in the future\n",
    "df.isForward = df.isForward.replace({True: 1, False: 0})\n",
    "df.isSucceeded = df.isSucceeded.replace({True: 1, False: 0})\n",
    "df.Team=df.Team.replace({'Team Forward Football_1':0,'Team Forward Football_2':1}) # first team 1-> number 1,but seems that it's better to have classified value begin from 0\n",
    "df['Pass type']=df['Pass type'].replace({\"Forward pass\":0,\"Lateral pass\":1,\"Backward pass\":2})\n",
    "df['Pressure level']=df['Pressure level'].replace({\"Full Pressure\":2,\"Limited Pressure\":1,\"No Pressure\":0})\n",
    "df['Zone']=df['Zone'].replace({'Attack':0,'Defence':1,'Mid field':2})\n",
    "df['Playing direction_first half']=df['Playing direction_first half'].replace({'left':0,'right':1})\n",
    "df['Playing direction_second half']=df['Playing direction_second half'].replace({'left':0,'right':1})\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Add features\n",
    "\n",
    "df['pass_x']=df[\"posX_passer\"]-df[\"received_PosX\"]\n",
    "df['pass_y']=df[\"posY_passer\"]-df[\"received_PosY\"]\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set1:  {95579, 95580, 95581, 95582, 95583, 95584, 95585, 95586, 95587, 95588, 95589, 95591, 95592, 95593, 95594, 95595, 95597, 95600, 95601, 95986, 95987, 95603, 95988, 95602}\n",
      "set2:  {95617, 95618, 95624, 95581, 95582, 95583, 95584, 95585, 95586, 95587, 95588, 95589, 95591, 95592, 95593, 95594, 95597, 95600, 95601, 95602, 95987, 95988, 95986}\n",
      "intersection: {95581, 95582, 95583, 95584, 95585, 95586, 95587, 95588, 95589, 95591, 95592, 95593, 95594, 95597, 95600, 95601, 95602, 95987, 95988, 95986}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "\n",
    "# evidence for using history of player\n",
    "set1 = set(df[df.Team==0]['Player_id'])\n",
    "set2= set(df[df.Team==1]['Player_id'])\n",
    "\n",
    "print(\"set1: \",set1)\n",
    "print(\"set2: \",set2)\n",
    "overlap = set1.intersection(set2)\n",
    "print(\"intersection:\",overlap)\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "\n",
    "# conclusion: from the sorted TimeStamp, I found that there are two matches rather than two teams in one match\n",
    "# -> more obvious, day is different\n",
    "# ->done\n",
    "# Also, information is not so much to be periodic-> split method should not use timeseriessplit\n",
    "\n",
    "df_team_one=df[df.Team==0]\n",
    "df_team_two=df[df.Team==1]\n",
    "\n",
    "# df_team_one.sort_values('TimeStamp') # check the starting and ending time of each match\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_team_two.sort_values('TimeStamp')\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "\n",
    "# df.groupby(['Player_id']).size() \n",
    "\n",
    "# calculate this in order to make sure there is no player with just a single line data\n",
    "# -> avoid when splitting dataset, there is no solution to split into training and test dataset\n",
    "# -> the result is positive-> done\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "\n",
    "def print_column_name(df):\n",
    "    for col in df:\n",
    "        print(\"column name:\",col)\n",
    "# print_column_name(df)\n",
    "\n",
    "\n",
    "# feature expansion\n",
    "# 1. this is obvious that the distance between passer and receicver has the influence on if the pass is successful\n",
    "# -> found that Pass length has been calculated->done\n",
    "# -> but angle of start is also informative-> angle is also calculated->done\n",
    "# 2. the difference between startTime and the start time have an influence on the physical strength-> done-> not informative, not peroidic->discard\n",
    "# 3. if the exact position or the interval of position has an influence on the result-> generate features-> how to use it\n",
    "#  * feature selection ( based on model, or correlation )\n",
    "#  * binned feature and then feature selection\n",
    "#  * alternative column: zone -> selected\n",
    "# 4. note that former part is more team 1, later part is for team 2, then there is also the overlap timestamp of records, try to use time to generate features-> not correct\n",
    "# time can also use Time block, or calculated time using minus -> select Time Block\n",
    "# 5. time related features\n",
    "# -> how many opponents appear in 5s in a exact scope-> don't have enough data\n",
    "# -> how many friends appear in 5s or around 5s in the same field-> don't have enough data\n",
    "# -> how many rival passes-> don't have enough data\n",
    "# -> how many friends passes-> done\n",
    "# -> how many rivals in a certain fields.-> don't have enough data\n",
    "# -> how many friends in a certain fields.-> don't have enough data\n",
    "\n",
    "# 5. Angle passe is for ridian, maybe degree is better -> not sure,but don't think so, just leave this idea here-> select ridian\n",
    "\n",
    "# consider the success rate of pass of one player-> should done after split->done\n",
    "\n",
    "# outlier?-> not suitable in this project\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Add features\n",
    "# How many friends passes in x second\n",
    "\n",
    "df.set_index('TimeStamp', drop=True, inplace=True)\n",
    "df = df.sort_index()\n",
    "window=df['Zone'].rolling('10s')\n",
    "\n",
    "def count_same_zone(x, current_player):\n",
    "    return x[x == current_player].count()\n",
    "\n",
    "df['player_num_in_same_zone'] = window.apply(lambda x: count_same_zone(x, x[0]), raw=False)\n",
    "df=df.reset_index()\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStamp                        0.000000\n",
       "Type                             0.000000\n",
       "posX_passer                      0.000000\n",
       "posY_passer                      0.000000\n",
       "received_PosX                    0.000000\n",
       "received_PosY                    0.000000\n",
       "isForward                        0.000000\n",
       "isSucceeded                      0.000000\n",
       "receiverId                       0.285464\n",
       "Player_id                        0.000000\n",
       "Team                             0.000000\n",
       "startTime                        0.000000\n",
       "matchDuration                    0.000000\n",
       "Club                             0.000000\n",
       "Time block                       0.000000\n",
       "Zone                             0.000000\n",
       "Area Football Pitch              0.000000\n",
       "Angle Passe                      0.000000\n",
       "Pass type                        0.000000\n",
       "Pass length                      0.005254\n",
       "Playing direction_first half     0.000000\n",
       "Playing direction_second half    0.000000\n",
       "x_pitchsize                      0.000000\n",
       "y_pitchsize                      0.000000\n",
       "Pressure level                   0.000000\n",
       "Distance to first opponent       0.000000\n",
       "Outpassed opponents              0.000000\n",
       "total_passes                     0.000000\n",
       "pass_x                           0.000000\n",
       "pass_y                           0.000000\n",
       "player_num_in_same_zone          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------- Observe: check missing values\n",
    "def get_none_percent(df):\n",
    "    return df.isna().sum()/df.shape[0]\n",
    "def get_none_num(df):\n",
    "    return df.isna().sum()\n",
    "\n",
    "get_none_percent(df)\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Fill missing value of Pass length\n",
    "# even the pass is not successful, then there is a expected sending point and a received point, so all the Pass length could be calculated\n",
    "# -> the missing value of Pass length is solved-> done\n",
    "\n",
    "def compute_lenght(df):\n",
    "    import math\n",
    "    length=np.sqrt((df['posX_passer']-df['received_PosX'])**2+(df['posY_passer']-df['received_PosY'])**2)\n",
    "    return length\n",
    "fill_series=compute_lenght(df)\n",
    "df[\"Pass length\"]=df[\"Pass length\"].fillna(fill_series)\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2854640980735552"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "# found that there is no receivedId if isSucceed is false, check if they have the one-to-one relation\n",
    "# -> the answer is yes, so receivedId is also the answer!!! we can't use it in training a model\n",
    "# -> just delete it (receivedId)->done\n",
    "\n",
    "1-df['isSucceeded'].sum()/df.shape[0]\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "# considering about the imbalance of isSucceeded\n",
    "\n",
    "# print(\"successful:\",df['isSucceeded'].sum()/df.shape[0])\n",
    "# print(\"failure:\",1-df['isSucceeded'].sum()/df.shape[0])\n",
    "\n",
    "# -> not so balanced\n",
    "# -> so try to interpolate the minor class or reduce the major class or using another score metric *** different choice\n",
    "# -> which is suitable? but imbalanced data is not series, so tring score metric, for example, f1 score->selected\n",
    "# *********************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "# Define a custom function that returns the data type of a column\n",
    "\n",
    "# def check_dtype(col):\n",
    "#     return col.dtype\n",
    "    \n",
    "# Apply the custom function to each column of the DataFrame\n",
    "# df.apply(check_dtype)\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Add features\n",
    "# generate bin values\n",
    "num_bins = 4\n",
    "df['angle_bins'] = pd.qcut(df['Angle Passe'], num_bins)\n",
    "\n",
    "lst=df['angle_bins'].unique()\n",
    "my_dic={}\n",
    "my_dict = {}\n",
    "for i, val in enumerate(lst):\n",
    "    my_dict[val] = i\n",
    "\n",
    "df.angle_bins = df.angle_bins.replace(my_dict).astype(\"int64\")\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Observe\n",
    "import seaborn as sns\n",
    "def get_distribution_of_each_column(df):\n",
    "    # Loop over the features\n",
    "    for col in df:\n",
    "        # Select the feature\n",
    "        feature = df[col]\n",
    "        \n",
    "        # Plot the distribution of the feature\n",
    "        sns.histplot(feature)\n",
    "        plt.show()\n",
    "\n",
    "def get_distribution_columns(df,col_lst):\n",
    "    # Loop over the features\n",
    "    for col in col_lst:\n",
    "        # Select the feature\n",
    "        feature = df[col]\n",
    "        \n",
    "        # Plot the distribution of the feature\n",
    "        sns.histplot(feature)\n",
    "        plt.show()\n",
    "# get_distribution_of_each_column(df) \n",
    "# found that Pass length and Distance to first oppoment have log distribution, also pair_count\n",
    "# pair_count is necesseay for transformation???-> selected not\n",
    "# *********************************************************\n",
    "\n",
    "# --------------------------------------------------------- Turn distribution\n",
    "# actually, this part should be put after all the features are generated, but I had done so, the result is the same\n",
    "# -> considering the interface, it's convenient to put it here\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "df['Pass length'] = transformer.fit_transform(df['Pass length'].values.reshape(-1,1))\n",
    "df['Distance to first opponent'] = transformer.fit_transform(df['Distance to first opponent'].values.reshape(-1,1))\n",
    "# df['pair_count'] = transformer.fit_transform(df['pair_count'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "# for validation\n",
    "# get_distribution_columns(df,['Pass length','Distance to first opponent','pair_count']) # found that normal distribution has been transformed successfully\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "df[\"receiverId\"]=df[\"receiverId\"].fillna(0) # a sign bit, in order to distinguish\n",
    "X = df.loc[:, ~df.columns.isin([\"isSucceeded\"])]\n",
    "y = df[\"isSucceeded\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df[['Player_id','isSucceeded']]) # former proven history variety, later proven target variety\n",
    "df_train=pd.concat([X_train, y_train], axis=1)\n",
    "df_test= pd.concat([X_test, y_test], axis=1)\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pair_count']=df.apply(use_lambda,axis=1)\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pair_count']=df.apply(use_lambda,axis=1)\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['succeed_count']=df['Player_id'].map(lambda x:pass_dict[x])\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['succeed_count']=df['Player_id'].map(lambda x:use_lambda(x))\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['succeed_rate']=df['Player_id'].map(lambda x:data_dict[x])\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['succeed_rate']=df['Player_id'].map(lambda x:use_lambda(x))\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['zone_rate']=df[['Player_id','Zone']].apply(use_zone_lambda_train,axis=1)\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['zone_rate']=df[['Player_id','Zone']].apply(use_zone_lambda_test,axis=1)\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pass_type_rate']=df[['Player_id','Pass type']].apply(use_pass_type_lambda_train,axis=1)\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pass_type_rate']=df[['Player_id','Pass type']].apply(use_pass_type_lambda_test,axis=1)\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pressure_level_rate']=df[['Player_id','Pressure level']].apply(use_pressure_level_lambda_train,axis=1)\n",
      "/var/folders/ws/p_92kz8j46lgdkjkzzrm9wm40000gn/T/ipykernel_23298/1032941593.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pressure_level_rate']=df[['Player_id','Pressure level']].apply(use_pressure_level_lambda_test,axis=1)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------- Add features\n",
    "# Note: the code segment is duplicate-> for future improvement !!!\n",
    "# count pair (receiverId and playId)\n",
    "def added_X_train_test_pair(df_train,X_train,X_test):\n",
    "    data_dict=df_train.groupby(['receiverId','Player_id']).sum('isSucceeded')['isSucceeded'].to_dict()\n",
    "\n",
    "    def use_lambda(df):\n",
    "        import math\n",
    "        if (df[\"receiverId\"],df['Player_id']) in data_dict.keys() and df[\"receiverId\"]!=0:\n",
    "            return data_dict[(df[\"receiverId\"],df['Player_id'])]\n",
    "        return 0\n",
    "\n",
    "    def add_pair_rate_train(df): \n",
    "        df['pair_count']=df.apply(use_lambda,axis=1)\n",
    "        return df\n",
    "\n",
    "    def add_pair_rate_test(df):\n",
    "        df['pair_count']=df.apply(use_lambda,axis=1)\n",
    "        return df\n",
    "\n",
    "    return add_pair_rate_train(X_train),add_pair_rate_test(X_test)\n",
    "    \n",
    "X_train,X_test=added_X_train_test_pair(df_train,X_train,X_test)\n",
    "# X_test.pair_count.sum() # result is more than 100, so a good idea at least\n",
    "\n",
    "def add_count_player_id(X_train,X_test):\n",
    "    pass_dict=df_train.groupby(['Player_id']).count()['posX_passer'].to_dict()\n",
    "    def add_success_count_train(df): # note player_id and success_rate is one vs one, have no information, but will have information on the test data!!!\n",
    "        df['succeed_count']=df['Player_id'].map(lambda x:pass_dict[x])\n",
    "        return df\n",
    "    def use_lambda(x):\n",
    "        import math\n",
    "        if x in pass_dict.keys():\n",
    "            return pass_dict[x]\n",
    "        return np.mean(list(pass_dict.values()))\n",
    "    def add_success_rate_test(df):\n",
    "        df['succeed_count']=df['Player_id'].map(lambda x:use_lambda(x))\n",
    "        return df\n",
    "    return add_success_count_train(X_train),add_success_rate_test(X_test)\n",
    "    \n",
    "X_train,X_test=add_count_player_id(X_train,X_test)\n",
    "\n",
    "def add_success_rate(df_train,X_train,X_test):\n",
    "    data_dict=df_train.groupby(['Player_id']).mean('isSucceeded')['isSucceeded'].to_dict()\n",
    "\n",
    "    def add_success_rate_train(df): # note player_id and success_rate is one vs one, have no information, but will have information on the test data!!!\n",
    "        df['succeed_rate']=df['Player_id'].map(lambda x:data_dict[x])\n",
    "        return df\n",
    "    \n",
    "    def use_lambda(x):\n",
    "        import math\n",
    "        if x in data_dict.keys():\n",
    "            return data_dict[x]\n",
    "        return np.mean(list(data_dict.values()))\n",
    "    def add_success_rate_test(df):\n",
    "        df['succeed_rate']=df['Player_id'].map(lambda x:use_lambda(x))\n",
    "        return df\n",
    "    return add_success_rate_train(X_train),add_success_rate_test(X_test)\n",
    "X_train,X_test=add_success_rate(df_train,X_train,X_test)\n",
    "\n",
    "def add_zone_rate(df_train,X_train,X_test):\n",
    "    # generate feature based on received_id and also zone\n",
    "    zone_dict=df_train.groupby(['Player_id','Zone']).mean('isSucceeded')['isSucceeded'].to_dict()\n",
    "    # df['zone_rate']=df[['Player_id','Zone']].map(lambda x:zone_dict[(x[0],x[1])]) this can't work, map can only used in series, use apply instead\n",
    "    def use_zone_lambda_train(df):\n",
    "        return zone_dict[(df[\"Player_id\"],df['Zone'])]\n",
    "    def add_zone_rate_train(df): # note player_id and success_rate is one vs one, have no information, but will have information on the test data!!!\n",
    "        df['zone_rate']=df[['Player_id','Zone']].apply(use_zone_lambda_train,axis=1)\n",
    "        return df\n",
    "\n",
    "    def use_zone_lambda_test(df):\n",
    "        import math\n",
    "        if (df[\"Player_id\"],df['Zone']) in zone_dict.keys():\n",
    "            return zone_dict[(df[\"Player_id\"],df['Zone'])]\n",
    "        else:\n",
    "            sub_dict = {key: value for key, value in zone_dict.items() if key[1]== df['Zone']}\n",
    "            return np.mean(list(sub_dict.values()))\n",
    "    def add_zone_rate_test(df):\n",
    "        df['zone_rate']=df[['Player_id','Zone']].apply(use_zone_lambda_test,axis=1)\n",
    "        return df\n",
    "    return add_zone_rate_train(X_train),add_zone_rate_test(X_test)\n",
    "X_train,X_test=add_zone_rate(df_train,X_train,X_test)\n",
    "\n",
    "def add_pass_type_rate(df_train,X_train,X_test):\n",
    "    pass_type_dict=df_train.groupby(['Player_id','Pass type']).mean('isSucceeded')['isSucceeded'].to_dict()\n",
    "\n",
    "    def use_pass_type_lambda_train(df):\n",
    "        return pass_type_dict[(df[\"Player_id\"],df['Pass type'])]\n",
    "    \n",
    "    def add_pass_type_rate_train(df): \n",
    "        df['pass_type_rate']=df[['Player_id','Pass type']].apply(use_pass_type_lambda_train,axis=1)\n",
    "        return df\n",
    "\n",
    "    def use_pass_type_lambda_test(df):\n",
    "        import math\n",
    "        if (df[\"Player_id\"],df['Pass type']) in pass_type_dict.keys():\n",
    "            return pass_type_dict[(df[\"Player_id\"],df['Pass type'])]\n",
    "        else:\n",
    "            sub_dict = {key: value for key, value in pass_type_dict.items() if key[1]== df['Pass type']}\n",
    "            return np.mean(list(sub_dict.values()))\n",
    "\n",
    "    def add_pass_type_rate_test(df):\n",
    "        df['pass_type_rate']=df[['Player_id','Pass type']].apply(use_pass_type_lambda_test,axis=1)\n",
    "        return df\n",
    "    return add_pass_type_rate_train(X_train),add_pass_type_rate_test(X_test)\n",
    "\n",
    "X_train,X_test=add_pass_type_rate(df_train,X_train,X_test)\n",
    "\n",
    "def add_pressure_level_rate(df_train,X_train,X_test):\n",
    "    pressure_level_dict=df_train.groupby(['Player_id','Pressure level']).mean('isSucceeded')['isSucceeded'].to_dict()\n",
    "\n",
    "    def use_pressure_level_lambda_train(df):\n",
    "        return pressure_level_dict[(df[\"Player_id\"],df['Pressure level'])]\n",
    "    \n",
    "    def add_pressure_level_rate_train(df): \n",
    "        df['pressure_level_rate']=df[['Player_id','Pressure level']].apply(use_pressure_level_lambda_train,axis=1)\n",
    "        return df\n",
    "\n",
    "    def use_pressure_level_lambda_test(df):\n",
    "        import math\n",
    "        if (df[\"Player_id\"],df['Pressure level']) in pressure_level_dict.keys():\n",
    "            return pressure_level_dict[(df[\"Player_id\"],df['Pressure level'])]\n",
    "        else:\n",
    "            sub_dict = {key: value for key, value in pressure_level_dict.items() if key[1]== df['Pressure level']}\n",
    "            return np.mean(list(sub_dict.values()))\n",
    "\n",
    "    def add_pressure_level_rate_test(df):\n",
    "        df['pressure_level_rate']=df[['Player_id','Pressure level']].apply(use_pressure_level_lambda_test,axis=1)\n",
    "        return df\n",
    "    return add_pressure_level_rate_train(X_train),add_pressure_level_rate_test(X_test)\n",
    "\n",
    "X_train,X_test=add_pressure_level_rate(df_train,X_train,X_test)\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_train_b=copy.deepcopy(X_train)\n",
    "X_test_b=copy.deepcopy(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=copy.deepcopy(X_train_b)\n",
    "X_test=copy.deepcopy(X_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Drop uninformative columns\n",
    "def drop_columns(df,lst_columns):\n",
    "    return df.drop(columns=lst_columns,axis=1)\n",
    "def drop_columns_with_one_unique_value(df):\n",
    "    return drop_columns(df,get_columns_with_one_unique_value(df))\n",
    "\n",
    "def get_columns_with_one_unique_value(df):\n",
    "    col_counts = df.nunique()\n",
    "    cols_with_one_unique_value = col_counts[col_counts == 1]\n",
    "    return list(cols_with_one_unique_value.index)\n",
    "\n",
    "lst_delete=['TimeStamp','startTime','Team','receiverId'] \n",
    "lst_one_value=get_columns_with_one_unique_value(df)\n",
    "X_train=drop_columns(X_train,lst_delete+lst_one_value)\n",
    "X_test=drop_columns(X_test,lst_delete+lst_one_value)\n",
    "# print(lst_one_value)\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- Show added features are informative\n",
    "def plot_corr(X_train,y_train):\n",
    "    import seaborn as sns\n",
    "    import copy\n",
    "    df_plot=copy.deepcopy(X_train)\n",
    "    df_plot['target'] = copy.deepcopy(y_train)\n",
    "    corr = df_plot.corr()\n",
    "    sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap=\"RdBu\")\n",
    "# plot_corr(X_train,y_train)\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8373983739837398\n",
      "Accuracy: 0.7202797202797203\n",
      "Precision: 0.7202797202797203\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------- model training\n",
    "from sklearn import metrics\n",
    "def get_dummy_score(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "    # Create an instance of the DummyClassifier class\n",
    "    dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "    # Fit the DummyClassifier instance to the training data\n",
    "    dummy_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred=dummy_classifier.predict(X_test)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "get_dummy_score(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# insight: classifiers are just above the dummy model or even above, try to extract more features\n",
    "\n",
    "# Define a dictionary of classification models\n",
    "models = {\n",
    "    \"logistic_regression\": LogisticRegression(),\n",
    "    \"support_vector_machine\": SVC(),\n",
    "    \"k_nearest_neighbors\": KNeighborsClassifier(),\n",
    "    \"decision_tree\": DecisionTreeClassifier(),\n",
    "    \"random_forest\": RandomForestClassifier(),\n",
    "    \"ada_boost\": AdaBoostClassifier(),\n",
    "    \"gradient_boosting\": GradientBoostingClassifier(),\n",
    "    \"xg_boost\": XGBClassifier(),\n",
    "    \"bagging\": BaggingClassifier(),\n",
    "    \"extra_trees\": ExtraTreesClassifier(),\n",
    "    \"mlp\": MLPClassifier(),\n",
    "    \"gaussian_process\": GaussianProcessClassifier(),\n",
    "    \"quadratic_discriminant_analysis\": QuadraticDiscriminantAnalysis()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluate_models(model,name, X_train, X_test, y_train, y_test ):\n",
    "  # Loop through each model\n",
    "  # Fit the model and make predictions\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Calculate and print the metrics\n",
    "  f1 = metrics.f1_score(y_test, y_pred)\n",
    "  acc = metrics.accuracy_score(y_test, y_pred)\n",
    "  prec = metrics.precision_score(y_test, y_pred)\n",
    "  print(name)\n",
    "  print(\"F1 Score:\", f1)\n",
    "  print(\"Accuracy:\", acc)\n",
    "  print(\"Precision:\", prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horus_liang/opt/anaconda3/envs/intro_to_ds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression\n",
      "F1 Score: 0.850356294536817\n",
      "Accuracy: 0.7797202797202797\n",
      "Precision: 0.8325581395348837\n",
      "support_vector_machine\n",
      "F1 Score: 0.8373983739837398\n",
      "Accuracy: 0.7202797202797203\n",
      "Precision: 0.7202797202797203\n",
      "k_nearest_neighbors\n",
      "F1 Score: 0.8449438202247191\n",
      "Accuracy: 0.7587412587412588\n",
      "Precision: 0.7866108786610879\n",
      "decision_tree\n",
      "F1 Score: 0.9242819843342037\n",
      "Accuracy: 0.8986013986013986\n",
      "Precision: 1.0\n",
      "random_forest\n",
      "F1 Score: 0.9242819843342037\n",
      "Accuracy: 0.8986013986013986\n",
      "Precision: 1.0\n",
      "ada_boost\n",
      "F1 Score: 0.9242819843342037\n",
      "Accuracy: 0.8986013986013986\n",
      "Precision: 1.0\n",
      "gradient_boosting\n",
      "F1 Score: 0.9242819843342037\n",
      "Accuracy: 0.8986013986013986\n",
      "Precision: 1.0\n",
      "xg_boost\n",
      "F1 Score: 0.9242819843342037\n",
      "Accuracy: 0.8986013986013986\n",
      "Precision: 1.0\n",
      "bagging\n",
      "F1 Score: 0.9242819843342037\n",
      "Accuracy: 0.8986013986013986\n",
      "Precision: 1.0\n",
      "extra_trees\n",
      "F1 Score: 0.9309462915601023\n",
      "Accuracy: 0.9055944055944056\n",
      "Precision: 0.9837837837837838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horus_liang/opt/anaconda3/envs/intro_to_ds/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp\n",
      "F1 Score: 0.0\n",
      "Accuracy: 0.27972027972027974\n",
      "Precision: 0.0\n",
      "gaussian_process\n",
      "F1 Score: 0.7780548628428928\n",
      "Accuracy: 0.6888111888111889\n",
      "Precision: 0.8\n",
      "quadratic_discriminant_analysis\n",
      "F1 Score: 0.921875\n",
      "Accuracy: 0.8951048951048951\n",
      "Precision: 0.9943820224719101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horus_liang/opt/anaconda3/envs/intro_to_ds/lib/python3.8/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# Loop through the models and evaluate each one\n",
    "for name, model in models.items():\n",
    "    evaluate_models(model,name,X_train, X_test, y_train, y_test)\n",
    "\n",
    "# note: majority of models, the score is bad than dummy model, which means there are so many noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horus_liang/opt/anaconda3/envs/intro_to_ds/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 600, 'max_depth': 40}\n",
      "F1 Score: 0.9151670951156812\n",
      "Accuracy: 0.8846153846153846\n",
      "Precision: 0.9726775956284153\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [600,610,620,590],\n",
    "    'max_depth': [40,50],\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create an instance of the Random Forest classifier\n",
    "classifier = ExtraTreesClassifier()\n",
    "\n",
    "# Create an instance of the RandomizedSearchCV class\n",
    "random_search = RandomizedSearchCV(classifier, param_grid,cv=5,scoring='f1') \n",
    "\n",
    "# Fit the RandomizedSearchCV to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(random_search.best_params_)\n",
    "\n",
    "classifier = random_search.best_estimator_\n",
    "y_pred=classifier.predict(X_test)\n",
    "# Calculate and print the metrics\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "prec = metrics.precision_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "\n",
    "# !!! conclusion: this kind of method is not correct, cv=ts_split, the best estimator would be the one tuning the parameter based on validation dataset,leading to overfitting, not generalization.\n",
    "# ->wrong\n",
    "# so note: maybe the model is too complex ?->discard this model->done\n",
    "# ->wrong\n",
    "# even through cv has test dataset, but still can overfit, becase overfit is the situation when strong model and less test score, less test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('intro_to_ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2d8aa6afe6024b62f6f9fc5a6755ecc5ab5ed657f1dbb49859cc1c431d03a86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
